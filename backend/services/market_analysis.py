"""
Market Analysis Service

åå°å¸‚åœºåˆ†ææœåŠ¡ï¼Œæ¯æ—¥æ‰§è¡Œæœç´¢å¹¶ä¿å­˜ç»“æœã€‚
ä¸æ˜¯ LangGraph èŠ‚ç‚¹ï¼Œè€Œæ˜¯ç‹¬ç«‹çš„åå°ä»»åŠ¡ã€‚
"""

from datetime import datetime, timezone, timedelta
from typing import Any, List
import random
import json

import structlog

from backend.services.model_router import ModelRouter, get_model_router
from backend.services.prompt_service import PromptService, get_prompt_service
from backend.services.database import DatabaseService, get_db_service
from backend.schemas.model_config import TaskType
from backend.tools.metaso_search import metaso_search

logger = structlog.get_logger(__name__)


class MarketAnalysisService:
    """
    å¸‚åœºåˆ†ææœåŠ¡

    åå°å®šæ—¶ä»»åŠ¡ä½¿ç”¨ï¼Œæ¯æ—¥æœç´¢çŸ­å‰§å¸‚åœºè¶‹åŠ¿å¹¶ä¿å­˜ã€‚
    """

    def __init__(
        self,
        model_router: ModelRouter = None,
        prompt_service: PromptService = None,
        db_service: DatabaseService = None,
    ):
        self.router = model_router or get_model_router()
        self.prompt_service = prompt_service or get_prompt_service()
        # DatabaseService å·²ä¿®å¤ event loop é—®é¢˜ï¼Œå¯ä»¥ç›´æ¥ç¼“å­˜
        self.db = db_service or get_db_service()

    async def _get_search_queries(self) -> List[str]:
        """
        åŠ¨æ€ç”Ÿæˆæœç´¢æŸ¥è¯¢ï¼ŒåŒ…å«åŸºç¡€æŸ¥è¯¢+è½®æ¢æŸ¥è¯¢

        ç­–ç•¥ï¼š
        1. åŸºç¡€æŸ¥è¯¢ï¼ˆæ¯æ—¥å¿…æœï¼‰ï¼šæ¦œå•ç±»
        2. è½®æ¢æŸ¥è¯¢ï¼ˆéšæœºé€‰æ‹©ï¼‰ï¼šé¢˜æè¶‹åŠ¿ã€ç¤¾ä¼šçƒ­ç‚¹ã€ç«å“åˆ†æ
        """
        # åŸºç¡€æŸ¥è¯¢ï¼ˆæ¯æ—¥å¿…æœï¼‰
        base_queries = [
            "2026å¹´çŸ­å‰§çƒ­åº¦æ¦œ æŠ–éŸ³å¿«æ‰‹ æ—¥æ¦œ",
            "2026å¹´çŸ­å‰§æ’­æ”¾é‡æ’è¡Œ å¾®ä¿¡è§†é¢‘å·",
            "2026å¹´çŸ­å‰§çˆ†æ¬¾ å°çº¢ä¹¦æ¨è",
        ]

        # é¢˜æè¶‹åŠ¿æŸ¥è¯¢æ± ï¼ˆè½®æ¢ï¼‰
        genre_query_pool = [
            "2026å¹´çŸ­å‰§æ–°å…´é¢˜æ æ— é™æµ è§„åˆ™æ€ªè°ˆ",
            "2026å¹´çŸ­å‰§åˆ›æ–°å…ƒç´  é“¶å‘ ç©¿è¶Š é‡ç”Ÿ",
            "2026å¹´çŸ­å‰§çƒ­é—¨äººè®¾ åå·®èŒ èº«ä»½é”™ä½",
            "2026å¹´çŸ­å‰§çƒ­é—¨èƒŒæ™¯ èµ›åšæœ‹å…‹ æœ«ä¸– ä»™ä¾ ",
            "2026å¹´çŸ­å‰§åˆ›æ–°æ¡ˆä¾‹ çˆ†æ¬¾åˆ†æ",
            "2026å¹´çŸ­å‰§é»‘é©¬ä½œå“ é€†è¢­",
            "2026å¹´çŸ­å‰§ niche å°ä¼—é¢˜æ",
        ]

        # ç¤¾ä¼šçƒ­ç‚¹æŸ¥è¯¢æ± ï¼ˆè½®æ¢ï¼‰
        social_query_pool = [
            "2026å¹´çƒ­é—¨è¯é¢˜ çŸ­å‰§æ”¹ç¼–",
            "2026å¹´ç½‘ç»œæµè¡Œè¯­ çŸ­å‰§",
            "2026å¹´ç¤¾ä¼šäº‹ä»¶ çŸ­å‰§åˆ›ä½œ",
            "2026å¹´æŠ–éŸ³çƒ­é—¨æŒ‘æˆ˜ çŸ­å‰§",
            "2026å¹´å¾®åšçƒ­æœ çŸ­å‰§",
        ]

        # ç«å“åˆ†ææŸ¥è¯¢æ± ï¼ˆè½®æ¢ï¼‰
        competitor_query_pool = [
            "2026å¹´çŸ­å‰§çˆ†æ¬¾å‰§å æ’­æ”¾é‡",
            "2026å¹´çŸ­å‰§çƒ­é—¨å‰§ å•†ä¸šæ¨¡å¼",
            "2026å¹´çŸ­å‰§åˆ›æ–°æ¡ˆä¾‹ è·å¥–ä½œå“",
            "2026å¹´çŸ­å‰§æ–°é”å¯¼æ¼” ä½œå“",
            "2026å¹´çŸ­å‰§å¹³å°ç«äº‰ æŠ–éŸ³å¿«æ‰‹",
        ]

        # éšæœºé€‰æ‹©ï¼Œç¡®ä¿å¤šæ ·æ€§
        selected_genre = random.sample(genre_query_pool, k=min(2, len(genre_query_pool)))
        selected_social = random.sample(social_query_pool, k=min(1, len(social_query_pool)))
        selected_competitor = random.sample(
            competitor_query_pool, k=min(1, len(competitor_query_pool))
        )

        all_queries = base_queries + selected_genre + selected_social + selected_competitor

        logger.info(
            "Generated search queries",
            base=len(base_queries),
            genre=len(selected_genre),
            social=len(selected_social),
            competitor=len(selected_competitor),
            total=len(all_queries),
        )

        return all_queries

    async def run_daily_analysis(self) -> dict[str, Any]:
        """
        æ‰§è¡Œæ¯æ—¥å¸‚åœºåˆ†æ

        1. æœç´¢çŸ­å‰§æ¦œå•å’Œè¶‹åŠ¿
        2. æå–çƒ­ç‚¹å…ƒç´ 
        3. LLM åˆ†ææ•°æ®
        4. ä¿å­˜åˆ°æ•°æ®åº“

        Returns:
            åˆ†æç»“æœå­—å…¸ï¼ˆåŒ…å«çƒ­ç‚¹å…ƒç´ ï¼‰
        """
        logger.info("Starting daily market analysis")

        try:
            # 1. æœç´¢å¸‚åœºæ•°æ®
            search_queries = await self._get_search_queries()

            search_results = []
            for query in search_queries:
                try:
                    result = await metaso_search(query)
                    search_results.append({"query": query, "result": result})
                    logger.info("Search completed", query=query, result_length=len(result))
                except Exception as e:
                    logger.error("Search failed", query=query, error=str(e))

            # 2. æå–çƒ­ç‚¹å…ƒç´ ï¼ˆæ–°å¢ï¼‰
            hot_elements = await self._extract_hot_elements(search_results)
            logger.info(
                "Extracted hot elements",
                tropes=len(hot_elements.get("hot_tropes", [])),
                emerging=len(hot_elements.get("emerging_combinations", [])),
                overused=len(hot_elements.get("overused_tropes", [])),
            )

            # 3. LLM åˆ†æï¼ˆä¼ å…¥çƒ­ç‚¹å…ƒç´ ï¼‰
            analysis = await self._analyze_with_llm(search_results, hot_elements)

            # å°†çƒ­ç‚¹å…ƒç´ åŠ å…¥åˆ†æç»“æœ
            analysis["hot_elements"] = hot_elements

            # 4. ä¿å­˜åˆ°æ•°æ®åº“
            await self._save_analysis(analysis)

            logger.info(
                "Daily market analysis completed",
                genre_count=len(analysis.get("genres", [])),
                hot_tropes_count=len(hot_elements.get("hot_tropes", [])),
            )

            return analysis

        except Exception as e:
            logger.error("Daily market analysis failed", error=str(e))
            raise

    async def _extract_hot_elements(self, search_results: list) -> dict:
        """
        ä½¿ç”¨LLMä»æœç´¢ç»“æœä¸­æå–å…·ä½“çš„çƒ­ç‚¹å…ƒç´ 

        æå–ï¼šçƒ­é—¨å…ƒç´ ã€æ–°å…´ç»„åˆã€è¿‡åº¦ä½¿ç”¨å¥—è·¯ã€å…·ä½“çˆ†æ¬¾å‰§å
        """
        from langchain_core.messages import HumanMessage, SystemMessage

        # æ„å»ºå®Œæ•´æœç´¢ä¸Šä¸‹æ–‡ï¼ˆå¢åŠ é•¿åº¦é™åˆ¶åˆ°3000å­—ç¬¦ï¼‰
        context = "\n\n".join(
            [f"æœç´¢: {r['query']}\nç»“æœ: {r['result'][:3000]}" for r in search_results]
        )

        # æå–æç¤ºè¯ï¼ˆä¼˜åŒ–ç‰ˆï¼šæ›´å¥½åœ°æ”¯æŒç»„åˆåˆ›æ–°ï¼‰
        extract_prompt = f"""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„çŸ­å‰§å¸‚åœºæ•°æ®æå–ä¸“å®¶ã€‚è¯·æ·±åº¦åˆ†æä»¥ä¸‹æœç´¢ç»“æœï¼Œæå–2026å¹´çŸ­å‰§å¸‚åœºçš„å…·ä½“çƒ­ç‚¹å…ƒç´ å’Œåˆ›æ–°è¶‹åŠ¿ã€‚

æœç´¢ç»“æœï¼š
{context}

è¯·æå–ä»¥ä¸‹ä¿¡æ¯ï¼ˆå¿…é¡»è¿”å›JSONæ ¼å¼ï¼‰ï¼š
{{
    "hot_tropes": ["å…ƒç´ 1", "å…ƒç´ 2", ...],  // æå–8-10ä¸ªå½“å‰æœ€çƒ­é—¨çš„å•å…ƒç´ ï¼ˆå¦‚ï¼š"èº«ä»½é”™ä½"ã€"æ— é™æµ"ã€"åæ´¾æ´—ç™½"ï¼‰
    "hot_settings": ["èƒŒæ™¯1", "èƒŒæ™¯2", ...],  // æå–5ä¸ªçƒ­é—¨èƒŒæ™¯è®¾å®šï¼ˆå¦‚ï¼š"ç°ä»£èŒåœº"ã€"æœ«ä¸–åºŸå¢Ÿ"ï¼‰
    "hot_character_types": ["äººè®¾1", "äººè®¾2", ...],  // æå–8ä¸ªçƒ­é—¨äººè®¾ç±»å‹ï¼ˆå¦‚ï¼š"éšè—å¤§ä½¬"ã€"åŒé‡äººæ ¼"ï¼‰
    "emerging_combinations": ["ç»„åˆ1", "ç»„åˆ2", ...],  // æå–5-8ä¸ªæ–°å…´é¢˜æ/å…ƒç´ ç»„åˆï¼ˆå¦‚ï¼š"æ— é™æµ+ç”œå® "ã€"èµ›åšæœ‹å…‹+åŒ»ç–—"ï¼‰
    "overused_tropes": ["å¥—è·¯1", "å¥—è·¯2", ...],  // æå–5ä¸ªå·²è¿‡åº¦ä½¿ç”¨çš„å¥—è·¯ï¼ˆå¦‚ï¼š"éœ¸é“æ€»è£çˆ±ä¸Šæˆ‘"ã€"é‡ç”Ÿå¤ä»‡æ‰“è„¸"ï¼‰
    "specific_works": ["å‰§å1", "å‰§å2", ...]  // æå–8-10ä¸ªå…·ä½“çš„çˆ†æ¬¾çŸ­å‰§åç§°
}}

é‡è¦è¦æ±‚ï¼š
1. **å…ƒç´ ä¸ç»„åˆçš„åŒºåˆ«**ï¼š
   - hot_tropesï¼šå•ä¸ªå…ƒç´ ï¼ˆå¦‚"ç©¿è¶Š"ã€"ç”œå® "ï¼‰
   - emerging_combinationsï¼šä¸¤ä¸ªæˆ–å¤šä¸ªå…ƒç´ çš„ç»„åˆï¼ˆå¦‚"ç©¿è¶Š+è™æ‹"ã€"æ— é™æµ+æ‹çˆ±"ï¼‰
   
2. **ç»„åˆåˆ›æ–°**ï¼š
   - ä»æœç´¢ç»“æœä¸­å‘ç°çœŸå®å­˜åœ¨çš„é¢˜æç»„åˆ
   - æ‰¾å‡º"Aé¢˜æ+Bé¢˜æ"çš„èåˆæ¡ˆä¾‹
   - æå–é‚£äº›"æ„æ–™ä¹‹å¤–ä½†æƒ…ç†ä¹‹ä¸­"çš„åˆ›æ–°æ­é…
   
3. **å…·ä½“æ¡ˆä¾‹**ï¼š
   - å¦‚æœæœç´¢æåˆ°ã€ŠXXå‰§ã€‹ï¼Œæå–å‰§åå’Œå®ƒçš„é¢˜æç»„åˆ
   - ä¾‹å¦‚ï¼šã€Šç©¿è¶Šåˆ°è™æ‹æ–‡çœ‹æˆ‘å¦‚ä½•è‡ªæ•‘ã€‹â†’ æå–ä¸º"ç©¿è¶Š+è™æ‹"ç»„åˆ
   
4. **è´¨é‡è¦æ±‚**ï¼š
   - å¿…é¡»æ˜¯å…·ä½“çš„ã€å¯æ“ä½œçš„å…ƒç´ 
   - æ¯ä¸ªå…ƒç´ /ç»„åˆä¸è¶…è¿‡15ä¸ªå­—
   - ä¼˜å…ˆæå–åˆ›æ–°å…ƒç´ å’Œç»„åˆ
   - overused_tropeså¿…é¡»æ˜¯å·²ç»å‡ºç°å¤šæ¬¡ã€è§‚ä¼—å®¡ç¾ç–²åŠ³çš„å¥—è·¯
   - specific_workså¿…é¡»æ˜¯çœŸå®å­˜åœ¨çš„çŸ­å‰§åç§°
   - ç¡®ä¿æ‰€æœ‰ä¿¡æ¯æ¥è‡ªæœç´¢ç»“æœï¼Œè€Œéç¼–é€ 

åªè¿”å›JSONï¼Œä¸è¦å…¶ä»–è§£é‡Šã€‚"""

        try:
            messages = [
                SystemMessage(content="ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„çŸ­å‰§å¸‚åœºæ•°æ®æå–åŠ©æ‰‹ã€‚åªè¿”å›JSONæ ¼å¼æ•°æ®ã€‚"),
                HumanMessage(content=extract_prompt),
            ]

            model = await self.router.get_model(
                user_id="system",
                task_type=TaskType.MARKET_ANALYST,
                project_id=None,
            )

            response = await model.ainvoke(messages)
            content = response.content

            # è§£æJSON
            import re

            # å°è¯•æå–JSON
            if "```json" in content:
                content = content.split("```json")[1].split("```")[0].strip()
            elif "```" in content:
                content = content.split("```")[1].split("```")[0].strip()

            # æ¸…ç†å¯èƒ½çš„é¢å¤–å­—ç¬¦
            content = content.strip()
            if content.startswith("{") and content.endswith("}"):
                parsed = json.loads(content)

                # ç¡®ä¿æ‰€æœ‰å­—æ®µå­˜åœ¨
                return {
                    "hot_tropes": parsed.get("hot_tropes", [])[:10],
                    "hot_settings": parsed.get("hot_settings", [])[:5],
                    "hot_character_types": parsed.get("hot_character_types", [])[:8],
                    "emerging_combinations": parsed.get("emerging_combinations", [])[:5],
                    "overused_tropes": parsed.get("overused_tropes", [])[:5],
                    "specific_works": parsed.get("specific_works", [])[:10],
                }

        except Exception as e:
            error_msg = str(e)
            logger.error(
                "Failed to extract hot elements",
                error=error_msg,
            )

        # å¦‚æœæå–å¤±è´¥ï¼Œè¿”å›éšæœºå›é€€æ•°æ®
        return self._generate_random_fallback()

    async def _analyze_with_llm(self, search_results: list, hot_elements: dict = {}) -> dict:
        """ä½¿ç”¨ LLM åˆ†ææœç´¢ç»“æœï¼ˆå¢å¼ºç‰ˆï¼ŒåŒ…å«çƒ­ç‚¹å…ƒç´ ï¼‰"""
        from langchain_core.messages import HumanMessage, SystemMessage

        # æ„å»ºæœç´¢ä¸Šä¸‹æ–‡ï¼ˆå¢åŠ é•¿åº¦åˆ°3000å­—ç¬¦ï¼‰
        context = "\n\n".join(
            [f"æœç´¢: {r['query']}\nç»“æœ: {r['result'][:3000]}" for r in search_results]
        )

        # æ„å»ºçƒ­ç‚¹å…ƒç´ ä¸Šä¸‹æ–‡
        hot_context = ""
        if hot_elements:
            hot_context = f"""

## å·²æå–çš„å¸‚åœºçƒ­ç‚¹å…ƒç´ ï¼ˆå¿…é¡»å‚è€ƒï¼‰

### ğŸ”¥ çƒ­é—¨å…ƒç´ 
{chr(10).join([f"- {trope}" for trope in hot_elements.get("hot_tropes", [])[:8]])}

### ğŸ†• æ–°å…´ç»„åˆ
{chr(10).join([f"- {combo}" for combo in hot_elements.get("emerging_combinations", [])[:5]])}

### ğŸš« å·²è¿‡åº¦ä½¿ç”¨çš„å…ƒç´ ï¼ˆåˆ†ææ—¶æ ‡æ˜ï¼‰
{chr(10).join([f"- {trope}" for trope in hot_elements.get("overused_tropes", [])[:5]])}

### ğŸ¬ å‚è€ƒçˆ†æ¬¾å‰§
{chr(10).join([f"- ã€Š{work}ã€‹" for work in hot_elements.get("specific_works", [])[:5]])}
"""

        # åŠ è½½ Prompt
        system_prompt = self.prompt_service.get_raw_prompt("market_analyst")

        # æ„å»ºæ¶ˆæ¯
        messages = [
            SystemMessage(content=system_prompt),
            HumanMessage(content=f"åˆ†æä»¥ä¸‹å¸‚åœºæ•°æ®ï¼š\n\n{context}\n\n{hot_context}"),
        ]

        # è°ƒç”¨ LLM
        model = await self.router.get_model(
            user_id="system",  # ç³»ç»Ÿä»»åŠ¡
            task_type=TaskType.MARKET_ANALYST,
            project_id=None,
        )

        response = await model.ainvoke(messages)
        content = response.content

        # è§£æ JSON
        return self._parse_analysis(content)

    def _generate_random_fallback(self) -> dict:
        """
        ç”Ÿæˆéšæœºå›é€€æ•°æ®ï¼ˆé¿å…ç¡¬ç¼–ç å›ºå®šåŒ–ï¼‰

        å½“å®æ—¶æœç´¢å¤±è´¥æ—¶ä½¿ç”¨ï¼Œä»å¤§çš„å€™é€‰æ± éšæœºé€‰æ‹©
        ç¡®ä¿æ¯æ¬¡è¿”å›ä¸åŒçš„å…ƒç´ ç»„åˆ
        """
        import random

        # æ‰©å±•çš„å€™é€‰æ± ï¼ˆç¡®ä¿å¤šæ ·æ€§ï¼‰
        candidate_tropes = [
            "èº«ä»½é”™ä½",
            "åå·®èŒ",
            "åŒé‡äººæ ¼",
            "é€†è¢­æˆé•¿",
            "éšè—å¤§ä½¬",
            "åæ´¾æ´—ç™½",
            "é‡‘æ‰‹æŒ‡",
            "ç³»ç»Ÿæµ",
            "ç©¿ä¹¦",
            "æ›¿èº«æ–‡å­¦",
            "ä¹…åˆ«é‡é€¢",
            "å…ˆå©šåçˆ±",
            "å¥‘çº¦å…³ç³»",
            "å¤±å¿†æ¢—",
            "çœŸå‡åƒé‡‘",
            "äº’æ¢èº«ä½“",
            "æ—¶é—´å¾ªç¯",
            "è¯»å¿ƒæœ¯",
            "é¢„çŸ¥æœªæ¥",
            "çµé­‚äº’æ¢",
        ]

        candidate_settings = [
            "ç°ä»£èŒåœº",
            "å¤ä»£å®«å»·",
            "æœ«ä¸–åºŸå¢Ÿ",
            "èµ›åšéƒ½å¸‚",
            "æ°‘å›½ä¸Šæµ·",
            "ä¿®ä»™ç•Œ",
            "æ ¡å›­é’æ˜¥",
            "è±ªé—¨ä¸–å®¶",
            "å¨±ä¹åœˆ",
            "ç¾é£Ÿè¡—",
            "åŒ»é™¢",
            "å¾‹æ‰€",
            "ç ”ç©¶æ‰€",
            "å†›è¥",
            "å¼‚èƒ½å­¦é™¢",
        ]

        candidate_characters = [
            "éœ¸æ€»",
            "èŒåœºæ–°äºº",
            "éšè—å¤§ä½¬",
            "åæ´¾æ´—ç™½",
            "è…¹é»‘ç”·ä¸»",
            "é£’çˆ½å¥³ä¸»",
            "ç—…å¨‡",
            "å¥¶ç‹—",
            "å¾¡å§",
            "å°é€æ˜",
            "å¤©æ‰å°‘å¹´",
            "åºŸæŸ´é€†è¢­",
            "åŒé‡èº«ä»½",
            "ç¥ç§˜æ¥å®¢",
            "å¤±å¿†è€…",
        ]

        candidate_combinations = [
            "æ— é™æµ+æ‹çˆ±",
            "èµ›åšæœ‹å…‹+åŒ»ç–—",
            "æœ«ä¸–+ç¾é£Ÿ",
            "è§„åˆ™æ€ªè°ˆ+æ ¡å›­",
            "ä½“è‚²+æ‚¬ç–‘",
            "åŒ»ç–—+ç”œå® ",
            "å•†æˆ˜+å¤ä»‡",
            "ä»™ä¾ +ç§‘å¹»",
            "ç©¿è¶Š+æ¢æ¡ˆ",
            "é‡ç”Ÿ+å•†æˆ˜",
            "å¨±ä¹åœˆ+ç³»ç»Ÿ",
            "ç¾é£Ÿ+æ²»æ„ˆ",
        ]

        candidate_overused = [
            "éœ¸é“æ€»è£çˆ±ä¸Šæˆ‘",
            "é‡ç”Ÿå¤ä»‡æ‰“è„¸",
            "è±ªé—¨æ©æ€¨",
            "çœŸå‡åƒé‡‘äº’æ’•",
            "è½¦ç¥¸å¤±å¿†",
            "è¯¯ä¼šåˆ†æ‰‹",
            "æ¶æ¯’å¥³é…",
            "ç™½è²èŠ±å¥³ä¸»",
        ]

        # éšæœºé€‰æ‹©ï¼Œç¡®ä¿æ¯æ¬¡è¿”å›éƒ½ä¸åŒ
        return {
            "hot_tropes": random.sample(candidate_tropes, k=min(6, len(candidate_tropes))),
            "hot_settings": random.sample(candidate_settings, k=min(4, len(candidate_settings))),
            "hot_character_types": random.sample(
                candidate_characters, k=min(6, len(candidate_characters))
            ),
            "emerging_combinations": random.sample(
                candidate_combinations, k=min(4, len(candidate_combinations))
            ),
            "overused_tropes": random.sample(candidate_overused, k=min(4, len(candidate_overused))),
            "specific_works": [],
            "_source": "random_fallback",  # æ ‡è®°è¿™æ˜¯å›é€€æ•°æ®
            "_note": "å®æ—¶æœç´¢å¤±è´¥ï¼Œä½¿ç”¨éšæœºå›é€€æ•°æ®ã€‚å»ºè®®é‡æ–°è¿è¡Œå¸‚åœºåˆ†æã€‚",
        }

    def _parse_analysis(self, content: str) -> dict:
        """è§£æ LLM è¿”å›çš„åˆ†æç»“æœ"""
        import json
        import re

        # å°è¯•æå– JSON
        try:
            if "```json" in content:
                json_str = content.split("```json")[1].split("```")[0].strip()
                return json.loads(json_str)

            content_stripped = content.strip()
            if content_stripped.startswith("{") and content_stripped.endswith("}"):
                return json.loads(content_stripped)

            match = re.search(r"\{[\s\S]*\}", content)
            if match:
                return json.loads(match.group())

        except (json.JSONDecodeError, IndexError):
            pass

        # å›é€€ï¼šè¿”å›é»˜è®¤
        return {
            "genres": [
                {
                    "id": "urban",
                    "name": "ç°ä»£éƒ½å¸‚",
                    "description": "èŒåœºã€çˆ±æƒ…ã€ç”Ÿæ´»",
                    "trend": "up",
                },
                {
                    "id": "revenge",
                    "name": "é€†è¢­å¤ä»‡",
                    "description": "æ‰“è„¸ã€çˆ½æ–‡ã€é‡ç”Ÿ",
                    "trend": "hot",
                },
                {
                    "id": "fantasy",
                    "name": "å¥‡å¹»ä»™ä¾ ",
                    "description": "ä¿®ä»™ã€ç„å¹»ã€ç³»ç»Ÿ",
                    "trend": "stable",
                },
            ],
            "tones": ["çˆ½æ„Ÿ", "ç”œå® ", "æ‚¬ç–‘", "æ²»æ„ˆ"],
            "insights": "åŸºäºå½“å‰å¸‚åœºè¶‹åŠ¿åˆ†æ",
            "audience": "18-35å²å¥³æ€§ç”¨æˆ·",
            "analyzed_at": datetime.now(timezone.utc).isoformat(),
        }

    async def _save_analysis(self, analysis: dict) -> None:
        """ä¿å­˜åˆ†æç»“æœåˆ°æ•°æ®åº“ï¼ˆå¢å¼ºç‰ˆï¼ŒåŒ…å«çƒ­ç‚¹å…ƒç´ ï¼‰"""
        try:
            from datetime import datetime, timedelta, timezone

            # âœ… ä¿®æ”¹ï¼šç¼“å­˜æœ‰æ•ˆæœŸä»7å¤©ç¼©çŸ­åˆ°1å¤©
            valid_until = datetime.now(timezone.utc) + timedelta(days=1)

            # æ„å»ºæ•°æ®ï¼ˆæ–°å¢ hot_elements å­—æ®µï¼‰
            data = {
                "report_type": "daily",  # æ”¹ä¸ºdailyï¼Œå› ä¸ºæ¯å¤©æ›´æ–°
                "genres": analysis.get("genres", []),
                "tones": analysis.get("tones", []),
                "insights": analysis.get("insights", ""),
                "target_audience": analysis.get("audience", ""),
                "search_queries": analysis.get("search_queries", []),
                "raw_search_results": analysis.get("raw_results", "")[:5000],
                "hot_elements": analysis.get("hot_elements", {}),  # æ–°å¢ï¼šçƒ­ç‚¹å…ƒç´ 
                "valid_until": valid_until.isoformat(),
                "is_active": True,
            }

            # æ’å…¥æ•°æ®åº“
            result = await self.db.create_market_report(data)
            logger.info(
                "Market analysis saved",
                report_id=result.get("id"),
                valid_until=valid_until.isoformat(),
                hot_tropes_count=len(analysis.get("hot_elements", {}).get("hot_tropes", [])),
            )

        except Exception as e:
            logger.error("Failed to save market analysis", error=str(e))
            # ä¿å­˜å¤±è´¥ä¸å½±å“ä¸»æµç¨‹

    async def run_quick_analysis(self) -> dict:
        """
        å¿«é€Ÿå¸‚åœºåˆ†æï¼ˆç”¨äºç¼“å­˜ç¼ºå¤±æ—¶ï¼‰

        åªæœç´¢2-3ä¸ªå…³é”®æŸ¥è¯¢ï¼Œå¿«é€Ÿæå–çƒ­ç‚¹å…ƒç´ ï¼Œä¸ç»è¿‡å®Œæ•´LLMåˆ†æ
        è€—æ—¶ï¼š3-5ç§’ï¼ˆvs å®Œæ•´åˆ†æçš„10-15ç§’ï¼‰
        """
        logger.info("Running quick market analysis...")

        try:
            # åªæœç´¢æœ€å…³é”®çš„2ä¸ªæŸ¥è¯¢
            quick_queries = [
                "2026å¹´çŸ­å‰§çƒ­é—¨å…ƒç´  çˆ†æ¬¾",
                "2026å¹´çŸ­å‰§æ–°å…´é¢˜æ åˆ›æ–°",
            ]

            search_results = []
            for query in quick_queries:
                try:
                    result = await metaso_search(query)
                    search_results.append({"query": query, "result": result})
                except Exception as e:
                    logger.warning("Quick search query failed", query=query, error=str(e))

            if not search_results:
                logger.warning("No quick search results, using fallback")
                return self._generate_random_fallback()

            # å¿«é€Ÿæå–çƒ­ç‚¹å…ƒç´ ï¼ˆä¸ç»è¿‡LLMï¼Œç›´æ¥è§£æï¼‰
            hot_elements = await self._extract_hot_elements(search_results)

            # æ„å»ºç®€åŒ–ç‰ˆåˆ†æç»“æœ
            quick_analysis = {
                "genres": [
                    {
                        "id": "trending",
                        "name": "å½“å‰çƒ­é—¨",
                        "description": "åŸºäºå®æ—¶æœç´¢",
                        "trend": "hot",
                    }
                ],
                "tones": ["çˆ½æ„Ÿ", "åˆ›æ–°", "åè½¬"],
                "insights": "åŸºäºå¿«é€Ÿå®æ—¶æœç´¢çš„å¸‚åœºçƒ­ç‚¹",
                "audience": "18-35å²",
                "hot_elements": hot_elements,
                "analyzed_at": datetime.now(timezone.utc).isoformat(),
                "_source": "quick_realtime",  # æ ‡è®°è¿™æ˜¯å¿«é€Ÿå®æ—¶æ•°æ®
            }

            logger.info(
                "Quick analysis completed", hot_tropes_count=len(hot_elements.get("hot_tropes", []))
            )

            return quick_analysis

        except Exception as e:
            logger.error("Quick analysis failed", error=str(e))
            # å¦‚æœå¿«é€Ÿåˆ†æä¹Ÿå¤±è´¥ï¼Œè¿”å›éšæœºå›é€€
            return self._generate_random_fallback()

    async def get_latest_analysis(self, allow_quick_realtime: bool = True) -> dict | None:
        """
        è·å–æœ€æ–°çš„æœ‰æ•ˆå¸‚åœºåˆ†æç»“æœï¼ˆå¢å¼ºç‰ˆï¼ŒåŒ…å«çƒ­ç‚¹å…ƒç´ ï¼‰

        Args:
            allow_quick_realtime: å¦‚æœç¼“å­˜è¿‡æœŸï¼Œæ˜¯å¦å…è®¸è§¦å‘å¿«é€Ÿå®æ—¶æœç´¢ï¼ˆé»˜è®¤Trueï¼‰
        """
        try:
            # æŸ¥è¯¢æœ€æ–°çš„æœ‰æ•ˆæŠ¥å‘Š
            report = await self.db.get_latest_market_report()

            if not report:
                logger.info("No cached market report found")
                return None

            # æ£€æŸ¥æ˜¯å¦è¿‡æœŸ
            from datetime import datetime, timezone

            valid_until = report.get("valid_until")
            if valid_until:
                if isinstance(valid_until, str):
                    valid_until = datetime.fromisoformat(valid_until.replace("Z", "+00:00"))

                if datetime.now(timezone.utc) > valid_until:
                    logger.info("Cached market report expired", valid_until=valid_until)
                    return None

            # âœ… è½¬æ¢ä¸ºæ ‡å‡†æ ¼å¼ï¼ˆæ–°å¢ hot_elementsï¼‰
            hot_elements = report.get("hot_elements", {})

            return {
                "genres": report.get("genres", []),
                "tones": report.get("tones", []),
                "insights": report.get("insights", ""),
                "audience": report.get("target_audience", ""),
                "hot_elements": hot_elements,  # æ–°å¢ï¼šçƒ­ç‚¹å…ƒç´ 
                "analyzed_at": report.get("created_at"),
                "report_id": report.get("id"),
                "valid_until": valid_until.isoformat() if valid_until else None,
            }

        except Exception as e:
            logger.error("Failed to get cached analysis", error=str(e))
            return None


# å…¨å±€æœåŠ¡å®ä¾‹
_market_analysis_service = None


def get_market_analysis_service() -> MarketAnalysisService:
    """è·å–å¸‚åœºåˆ†ææœåŠ¡å®ä¾‹"""
    global _market_analysis_service
    if _market_analysis_service is None:
        _market_analysis_service = MarketAnalysisService()
    return _market_analysis_service
