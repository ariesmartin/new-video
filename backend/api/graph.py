"""
Graph API Routes

Endpoints for the LangGraph workflow system
"""

from fastapi import APIRouter, HTTPException, BackgroundTasks, Query
from pydantic import BaseModel, Field
from typing import Optional, Dict, Any, List
import uuid
import structlog
import json

from datetime import datetime
from backend.graph.main_graph import get_graph_for_request
from backend.schemas.agent_state import create_initial_state
from backend.services.chat_init_service import (
    is_cold_start_message,
    create_welcome_message,
    get_content_status,
    prepare_initial_state,
)

logger = structlog.get_logger(__name__)

router = APIRouter(prefix="/api/graph", tags=["graph"])


class ActionButton(BaseModel):
    """æ“ä½œæŒ‰é’®"""

    label: str
    action: str
    payload: Dict[str, Any] = Field(default_factory=dict)
    style: str = "primary"  # primary/secondary/danger/ghost
    icon: Optional[str] = None
    disabled: bool = False
    disabled_reason: Optional[str] = None


class UIInteractionBlock(BaseModel):
    """UI äº¤äº’å—"""

    block_type: str = "action_group"  # action_group/selection/confirmation/input
    title: Optional[str] = None
    description: Optional[str] = None
    buttons: List[ActionButton] = Field(default_factory=list)
    data: Dict[str, Any] = Field(default_factory=dict)
    dismissible: bool = True


class ContentStatus(BaseModel):
    """å†…å®¹çŠ¶æ€"""

    has_novel_content: bool = False
    has_script: bool = False
    has_storyboard: bool = False
    has_any_content: bool = False


class ChatRequest(BaseModel):
    """Chat request payload"""

    user_id: str
    project_id: Optional[str] = None
    session_id: Optional[str] = None
    message: Optional[str] = None
    action: Optional[str] = None  # e.g., "cold_start", "random_plan", "continue"
    context: Optional[Dict[str, Any]] = Field(default_factory=dict)


class ChatResponse(BaseModel):
    """Chat response"""

    routed_agent: Optional[str] = None
    workflow_plan: Optional[List[Dict[str, Any]]] = None
    ui_feedback: Optional[str] = None
    intent_analysis: Optional[str] = None
    messages: Optional[List[Dict[str, Any]]] = None
    ui_interaction: Optional[UIInteractionBlock] = None  # æ–°å¢ï¼šUIäº¤äº’å—
    is_cold_start: bool = False  # æ–°å¢ï¼šæ˜¯å¦æ˜¯å†·å¯åŠ¨
    content_status: Optional[ContentStatus] = None  # æ–°å¢ï¼šå†…å®¹çŠ¶æ€


class ChatInitRequest(BaseModel):
    """èŠå¤©åˆå§‹åŒ–è¯·æ±‚ - åç«¯å†³å®šè¿”å›å†å²è¿˜æ˜¯å†·å¯åŠ¨"""

    user_id: str
    project_id: str
    session_id: Optional[str] = None


class ChatMessage(BaseModel):
    """ç»Ÿä¸€çš„æ¶ˆæ¯æ ¼å¼"""

    id: str
    role: str  # 'user' | 'assistant' | 'system'
    content: str
    display_label: Optional[str] = None  # å‹å¥½æ˜¾ç¤ºæ ‡ç­¾ï¼ˆç”¨äº action æ¶ˆæ¯ï¼‰
    timestamp: str
    ui_interaction: Optional[UIInteractionBlock] = None


class ChatInitResponse(BaseModel):
    """èŠå¤©åˆå§‹åŒ–å“åº” - ç»Ÿä¸€æ ¼å¼"""

    thread_id: str
    messages: List[ChatMessage]
    is_cold_start: bool  # true=å†·å¯åŠ¨ï¼ˆæ–°ä¼šè¯ï¼‰ï¼Œfalse=æ¢å¤å†å²
    ui_interaction: Optional[UIInteractionBlock] = None  # å†·å¯åŠ¨æ—¶çš„ UI ç»„ä»¶


@router.post("/chat", response_model=ChatResponse)
async def chat_endpoint(request: ChatRequest):
    """
    Main chat endpoint for the LangGraph workflow system.

    Handles:
    - Cold start (welcome message with function entry buttons)
    - Single-step workflows
    - Multi-step workflows
    """
    try:
        logger.info(
            "Chat endpoint called",
            user_id=request.user_id,
            action=request.action,
            has_message=bool(request.message),
        )

        # Generate IDs if not provided
        project_id = request.project_id or str(uuid.uuid4())
        session_id = request.session_id or str(uuid.uuid4())

        # Create initial state
        state = create_initial_state(
            user_id=request.user_id,
            project_id=project_id,
            thread_id=session_id,
        )

        # Add context
        if request.context:
            for key, value in request.context.items():
                state[key] = value

        # ===== æ­£å¸¸æµç¨‹ï¼šæ·»åŠ ç”¨æˆ·æ¶ˆæ¯ =====
        if request.message:
            from langchain_core.messages import HumanMessage

            state["messages"] = [HumanMessage(content=request.message)]
            state["user_input"] = request.message

        # Handle action-based routing
        if request.action == "random_plan":
            # AI Random Plan - route to story_planner for automatic plan generation
            logger.info("Action: random_plan - Routing to story_planner")
            state["routed_agent"] = "story_planner"
            state["ui_feedback"] = "æ­£åœ¨ä¸ºæ‚¨ç”ŸæˆAIéšæœºæ–¹æ¡ˆ..."

        elif request.action == "continue":
            # Continue existing workflow
            logger.info("Action: continue - Resuming workflow")
            # The graph will handle continuation
            pass

        # Get graph for request
        graph = await get_graph_for_request()

        # Prepare config for checkpointer
        config = {
            "configurable": {
                "thread_id": session_id,
            }
        }

        # Run the graph (invoke with initial state and config)
        result = await graph.ainvoke(state, config)

        # è·å–å†…å®¹çŠ¶æ€
        content_status = get_content_status(result)

        # Extract response data
        response_data = {
            "routed_agent": result.get("routed_agent"),
            "workflow_plan": result.get("workflow_plan", []),
            "ui_feedback": result.get("ui_feedback"),
            "intent_analysis": result.get("intent_analysis"),
            "messages": [
                {"type": type(m).__name__, "content": m.content} for m in result.get("messages", [])
            ]
            if result.get("messages")
            else None,
            "ui_interaction": UIInteractionBlock(**result["ui_interaction"].dict())
            if result.get("ui_interaction")
            else None,
            "is_cold_start": False,
            "content_status": ContentStatus(**content_status),
        }

        logger.info(
            "Chat endpoint completed",
            routed_agent=response_data["routed_agent"],
            has_workflow_plan=bool(response_data["workflow_plan"]),
        )

        return ChatResponse(**response_data)

    except RuntimeError as e:
        # æ¨¡å‹æœªé…ç½®é”™è¯¯
        error_msg = str(e)
        if "æœªé…ç½®æ¨¡å‹æ˜ å°„" in error_msg:
            logger.warning("Model not configured", user_id=request.user_id, error=error_msg)
            raise HTTPException(
                status_code=400,
                detail={
                    "error": "MODEL_NOT_CONFIGURED",
                    "message": error_msg,
                    "action_required": "è¯·å‰å¾€è®¾ç½® -> LLM æœåŠ¡å•†é…ç½®æ¨¡å‹æ˜ å°„",
                },
            )
        logger.error("Runtime error", error=error_msg)
        raise HTTPException(status_code=500, detail=error_msg)
    except Exception as e:
        logger.error("Chat endpoint error", error=str(e))
        raise HTTPException(status_code=500, detail=str(e))


@router.post("/chat/init", response_model=ChatInitResponse)
async def chat_init_endpoint(request: ChatInitRequest):
    """
    èŠå¤©åˆå§‹åŒ–æ¥å£ - åç«¯å†³å®šè¿”å›å†å²è®°å½•æˆ–å†·å¯åŠ¨æ¬¢è¿æ¶ˆæ¯

    è¿™æ˜¯å‰ç«¯åº”è¯¥è°ƒç”¨çš„å”¯ä¸€åˆå§‹åŒ–æ¥å£ï¼š
    1. åç«¯æ£€æŸ¥æ˜¯å¦æœ‰å†å²è®°å½•
    2. å¦‚æœæœ‰ï¼šè¿”å›å†å²æ¶ˆæ¯åˆ—è¡¨
    3. å¦‚æœæ²¡æœ‰ï¼šè¿”å›å†·å¯åŠ¨æ¬¢è¿æ¶ˆæ¯ + UI æŒ‰é’®

    å‰ç«¯åªéœ€è¦ï¼šè°ƒç”¨æ­¤æ¥å£ â†’ æ˜¾ç¤ºè¿”å›çš„æ¶ˆæ¯
    """
    try:
        logger.info(
            "Chat init endpoint called",
            user_id=request.user_id,
            project_id=request.project_id,
        )

        # ç”Ÿæˆæˆ–å¤ç”¨ session_id ä½œä¸º thread_id
        thread_id = request.session_id or f"thread-{uuid.uuid4()}"

        # ä» checkpointer æŸ¥è¯¢å†å²è®°å½•
        from backend.graph.checkpointer import get_or_create_checkpointer, checkpointer_manager
        from langchain_core.messages import HumanMessage, AIMessage

        checkpointer, conn = await get_or_create_checkpointer()
        config = {"configurable": {"thread_id": thread_id}}

        def format_message_content(content) -> str:
            """å°†æ¶ˆæ¯å†…å®¹è½¬æ¢ä¸ºå‹å¥½æ ¼å¼ï¼Œå¤„ç† action JSON å’Œ Master Router JSON"""
            if not content:
                return ""

            content_str = str(content).strip()

            # Action åˆ°å‹å¥½æ ‡ç­¾çš„æ˜ å°„ï¼ˆç”¨äºç”¨æˆ·æ¶ˆæ¯ï¼‰
            action_labels = {
                "start_creation": "ğŸ¬ å¼€å§‹åˆ›ä½œ",
                "adapt_script": "ğŸ“œ å‰§æœ¬æ”¹ç¼–",
                "create_storyboard": "ğŸ¨ åˆ†é•œåˆ¶ä½œ",
                "inspect_assets": "ğŸ‘¤ èµ„äº§æ¢æŸ¥",
                "random_plan": "ğŸ² éšæœºæ–¹æ¡ˆ",
                "select_genre": "ğŸ¯ é€‰æ‹©èµ›é“",
                "start_custom": "âœ¨ è‡ªç”±åˆ›ä½œ",
                "reset_genre": "ğŸ”™ é‡é€‰èƒŒæ™¯",
                "select_plan": "ğŸ“‹ é€‰æ‹©æ–¹æ¡ˆ",
                "proceed_to_planning": "ğŸ¤– AI è‡ªåŠ¨é€‰é¢˜",
                "cold_start": "ğŸš€ å¯åŠ¨åŠ©æ‰‹",
            }

            # 1. å°è¯•è§£æ action JSONï¼ˆç”¨æˆ·æ¶ˆæ¯ï¼‰
            if content_str.startswith("{") and '"action"' in content_str:
                try:
                    parsed = json.loads(content_str)
                    action = parsed.get("action") if parsed else None
                    if action and isinstance(action, str):
                        label = action_labels.get(action) or action
                        # å¦‚æœæœ‰ genreï¼Œæ·»åŠ åˆ°æ ‡ç­¾
                        if parsed.get("payload", {}).get("genre"):
                            genre = parsed["payload"]["genre"]
                            if genre:
                                label = f"{label} ({genre})"
                        return label
                except (json.JSONDecodeError, KeyError, TypeError):
                    pass

            # 2. å°è¯•è§£æ Master Router JSONï¼ˆAI æ¶ˆæ¯ï¼‰
            # æ ¼å¼: {"thought_process": "...", "target_agent": "...", "ui_feedback": "..."}
            if content_str.startswith("{") and (
                '"ui_feedback"' in content_str or '"thought_process"' in content_str
            ):
                try:
                    parsed = json.loads(content_str)
                    if parsed and isinstance(parsed, dict):
                        # ä¼˜å…ˆæå– ui_feedback
                        ui_feedback = parsed.get("ui_feedback")
                        if ui_feedback and isinstance(ui_feedback, str) and ui_feedback.strip():
                            return ui_feedback.strip()

                        # å¦‚æœæ²¡æœ‰ ui_feedbackï¼Œå°è¯•æå– thought_process
                        thought_process = parsed.get("thought_process")
                        if (
                            thought_process
                            and isinstance(thought_process, str)
                            and thought_process.strip()
                        ):
                            return thought_process.strip()
                except (json.JSONDecodeError, TypeError):
                    pass

            return content_str

        # ä» checkpointer åŠ è½½å†å²è®°å½•
        history_messages = []
        channel_values = None
        saved_ui_interaction = None

        try:
            if checkpointer:
                read_config = {"configurable": {"thread_id": thread_id}}
                checkpoint = await checkpointer.aget(read_config)

                if checkpoint:
                    channel_values = checkpoint.get("channel_values", {})

                    # æ¶ˆæ¯å·²ç»ç”± JsonPlusSerializer è‡ªåŠ¨ååºåˆ—åŒ–ä¸ºæ¶ˆæ¯å¯¹è±¡
                    # ä¸éœ€è¦æ‰‹åŠ¨è°ƒç”¨ messages_from_dict

                    # è·å– ui_interaction
                    saved_ui_interaction = channel_values.get("ui_interaction")

            # æ£€æŸ¥æ˜¯å¦æˆåŠŸåŠ è½½äº†æ¶ˆæ¯
            if channel_values and "messages" in channel_values:
                raw_messages = channel_values["messages"]

                # è½¬æ¢ LangChain æ¶ˆæ¯ä¸º ChatMessage æ ¼å¼
                for idx, msg in enumerate(raw_messages):
                    # å¤„ç† LangChain æ¶ˆæ¯å¯¹è±¡
                    if isinstance(msg, (HumanMessage, AIMessage)):
                        role = "user" if isinstance(msg, HumanMessage) else "assistant"
                        formatted_content = format_message_content(str(msg.content))
                        
                        # ä»æ¶ˆæ¯çš„ additional_kwargs ä¸­æå– ui_interaction
                        # è¿™æ˜¯æœ€å¯é çš„æ¥æºï¼Œå› ä¸º SDUI åœ¨åˆ›å»ºæ—¶å°±åµŒå…¥äº†æ¶ˆæ¯ä¸­
                        msg_ui_interaction = None
                        if hasattr(msg, 'additional_kwargs') and msg.additional_kwargs:
                            ui_data = msg.additional_kwargs.get('ui_interaction')
                            if ui_data:
                                try:
                                    if isinstance(ui_data, UIInteractionBlock):
                                        msg_ui_interaction = ui_data
                                    elif isinstance(ui_data, dict):
                                        msg_ui_interaction = UIInteractionBlock(**ui_data)
                                except Exception as e:
                                    logger.warning(f"Failed to parse ui_interaction from additional_kwargs: {e}")
                        
                    # å¤„ç† dict æ ¼å¼æ¶ˆæ¯ (ä» checkpoint åŠ è½½çš„åŸå§‹æ ¼å¼)
                    elif isinstance(msg, dict):
                        msg_ui_interaction = None
                        # LangChain message_to_dict æ ¼å¼: {"type": "ai", "data": {"content": "..."}}
                        if "type" in msg and "data" in msg:
                            msg_type = msg.get("type", "")
                            msg_data = msg.get("data", {})
                            role = "user" if msg_type == "human" else "assistant"
                            content = (
                                msg_data.get("content", "")
                                if isinstance(msg_data, dict)
                                else str(msg_data)
                            )
                            formatted_content = format_message_content(content)
                            
                            # ä» data.additional_kwargs ä¸­æå– ui_interaction
                            if isinstance(msg_data, dict):
                                ui_data = msg_data.get('additional_kwargs', {}).get('ui_interaction')
                                if ui_data:
                                    try:
                                        if isinstance(ui_data, UIInteractionBlock):
                                            msg_ui_interaction = ui_data
                                        elif isinstance(ui_data, dict):
                                            msg_ui_interaction = UIInteractionBlock(**ui_data)
                                    except Exception:
                                        pass
                        # ç®€å•æ ¼å¼: {"role": "assistant", "content": "..."}
                        elif "role" in msg:
                            role = msg.get("role", "assistant")
                            formatted_content = format_message_content(str(msg.get("content", "")))
                        else:
                            continue  # æ— æ³•è¯†åˆ«çš„æ ¼å¼ï¼Œè·³è¿‡
                    else:
                        continue  # æ— æ³•è¯†åˆ«çš„ç±»å‹ï¼Œè·³è¿‡

                    # å¦‚æœæ¶ˆæ¯æœ¬èº«æ²¡æœ‰ ui_interactionï¼Œå°è¯•ä½¿ç”¨å…¨å±€ä¿å­˜çš„ ui_interaction
                    # ä½†åªä¸ºæ¬¢è¿æ¶ˆæ¯ï¼ˆç¬¬ä¸€æ¡ AI æ¶ˆæ¯ï¼‰é™„åŠ 
                    ui_interaction_data = msg_ui_interaction
                    if not ui_interaction_data and idx == 0 and role == "assistant" and saved_ui_interaction:
                        try:
                            if isinstance(saved_ui_interaction, UIInteractionBlock):
                                ui_interaction_data = saved_ui_interaction
                            elif isinstance(saved_ui_interaction, dict):
                                ui_interaction_data = UIInteractionBlock(**saved_ui_interaction)
                        except Exception as e:
                            logger.warning(f"Failed to parse saved_ui_interaction: {e}")

                    history_messages.append(
                        ChatMessage(
                            id=f"msg-{thread_id}-{idx}",
                            role=role,
                            content=formatted_content,
                            timestamp=datetime.now().isoformat(),
                            ui_interaction=ui_interaction_data,
                        )
                    )
        except Exception as e:
            logger.warning(
                "Failed to load history from checkpointer", thread_id=thread_id, error=str(e)
            )
        finally:
            # å½’è¿˜æ•°æ®åº“è¿æ¥åˆ°è¿æ¥æ± 
            if conn:
                try:
                    await checkpointer_manager._pool.putconn(conn)
                except Exception as e:
                    logger.warning("Failed to return connection to pool", error=str(e))

        # å¦‚æœæœ‰å†å²è®°å½•ï¼Œè¿”å›å†å²æ¶ˆæ¯
        if history_messages:
            logger.info(
                "History found, returning chat history",
                thread_id=thread_id,
                message_count=len(history_messages),
            )
            return ChatInitResponse(
                thread_id=thread_id,
                messages=history_messages,
                is_cold_start=False,
                ui_interaction=None,
            )

        # æ— å†å²è®°å½•ï¼Œè§¦å‘å†·å¯åŠ¨
        # é‡è¦ä¿®å¤: é€šè¿‡ LangGraph æ­£å¸¸æµç¨‹è¿è¡Œå†·å¯åŠ¨ï¼Œè®© JsonPlusSerializer è‡ªåŠ¨å¤„ç†æ¶ˆæ¯åºåˆ—åŒ–
        # è€Œä¸æ˜¯æ‰‹åŠ¨ä¿å­˜ checkpointï¼Œè¿™æ ·å¯ä»¥ä¿è¯æ¶ˆæ¯æ ¼å¼æ­£ç¡®
        logger.info("No history found, triggering cold start via LangGraph")

        # åˆ›å»ºåˆå§‹çŠ¶æ€
        state = create_initial_state(
            user_id=request.user_id,
            project_id=request.project_id,
            thread_id=thread_id,
        )

        # æ ‡è®°ä¸ºå†·å¯åŠ¨ï¼Œè®©å›¾è·¯ç”±åˆ° cold_start èŠ‚ç‚¹
        state["is_cold_start"] = True
        state["messages"] = []  # ç¡®ä¿ messages ä¸ºç©ºï¼Œè§¦å‘å†·å¯åŠ¨è·¯ç”±

        # é€šè¿‡ LangGraph æ­£å¸¸æµç¨‹è¿è¡Œå†·å¯åŠ¨
        graph = await get_graph_for_request()
        config = {
            "configurable": {
                "thread_id": thread_id,
            }
        }

        # è¿è¡Œå›¾ - LangGraph ä¼šè‡ªåŠ¨ä¿å­˜ checkpointï¼Œä½¿ç”¨ JsonPlusSerializer æ­£ç¡®åºåˆ—åŒ–æ¶ˆæ¯
        result = await graph.ainvoke(state, config)

        # ä»ç»“æœä¸­è·å–æ¬¢è¿æ¶ˆæ¯å’Œ UI interaction
        result_messages = result.get("messages", [])
        ui_interaction = result.get("ui_interaction")

        # æ„å»ºè¿”å›çš„æ¶ˆæ¯åˆ—è¡¨
        welcome_messages = []
        for idx, msg in enumerate(result_messages):
            # å¤„ç† LangChain æ¶ˆæ¯å¯¹è±¡
            role = "assistant" if hasattr(msg, "type") and msg.type == "ai" else "user"
            content = msg.content if hasattr(msg, "content") else str(msg)
            
            # æ„å»º UI interaction block
            ui_block = None
            if idx == len(result_messages) - 1 and ui_interaction:
                try:
                    if hasattr(ui_interaction, "dict"):
                        ui_block = UIInteractionBlock(**ui_interaction.dict())
                    elif isinstance(ui_interaction, dict):
                        ui_block = UIInteractionBlock(**ui_interaction)
                except Exception as e:
                    logger.warning("Failed to parse ui_interaction", error=str(e))
            
            welcome_messages.append(
                ChatMessage(
                    id=f"welcome-{uuid.uuid4()}",
                    role=role,
                    content=content,
                    timestamp=datetime.now().isoformat(),
                    ui_interaction=ui_block,
                )
            )

        # å¦‚æœæ²¡æœ‰æ¶ˆæ¯ï¼Œåˆ›å»ºä¸€ä¸ªé»˜è®¤æ¬¢è¿æ¶ˆæ¯
        if not welcome_messages:
            logger.warning("No messages from cold start, creating default welcome")
            welcome_messages.append(
                ChatMessage(
                    id=f"welcome-{uuid.uuid4()}",
                    role="assistant",
                    content="æ¬¢è¿ä½¿ç”¨ AI åˆ›ä½œåŠ©æ‰‹ï¼è¯·ä»ä¸‹æ–¹é€‰æ‹©åŠŸèƒ½å…¥å£ã€‚",
                    timestamp=datetime.now().isoformat(),
                    ui_interaction=None,
                )
            )

        logger.info(
            "Cold start completed via LangGraph",
            thread_id=thread_id,
            message_count=len(welcome_messages),
        )

        return ChatInitResponse(
            thread_id=thread_id,
            messages=welcome_messages,
            is_cold_start=True,
            ui_interaction=welcome_messages[-1].ui_interaction if welcome_messages else None,
        )

    except Exception as e:
        logger.error("Chat init endpoint error", error=str(e))
        raise HTTPException(status_code=500, detail=f"åˆå§‹åŒ–å¤±è´¥: {str(e)}")


@router.get("/health")
async def graph_health_check():
    """Health check for graph system"""
    return {
        "status": "ok",
        "component": "graph",
        "version": "4.1.0",
        "features": ["workflow_plan", "multi_step", "agent_registry", "cold_start", "chat_init"],
    }


@router.get("/messages/{thread_id}")
async def get_chat_messages(
    thread_id: str,
    user_id: str = Query(..., description="ç”¨æˆ·ID"),
):
    """
    è·å–èŠå¤©å†å²æ¶ˆæ¯

    ä» checkpointer ä¸­è·å–æŒ‡å®š thread çš„æ‰€æœ‰æ¶ˆæ¯å†å²
    """
    try:
        from backend.graph.checkpointer import get_or_create_checkpointer, checkpointer_manager

        checkpointer, conn = await get_or_create_checkpointer()

        try:
            # æŸ¥è¯¢ checkpoint
            config = {"configurable": {"thread_id": thread_id}}
            checkpoint = await checkpointer.aget(config)

            messages = []
            if checkpoint:
                channel_values = checkpoint.get("channel_values", {})
                if channel_values and "messages" in channel_values:
                    raw_messages = channel_values["messages"]
                    # è½¬æ¢æ¶ˆæ¯æ ¼å¼ - å¤„ç†æ¶ˆæ¯å¯¹è±¡å’Œ dict æ ¼å¼
                    for msg in raw_messages:
                        if isinstance(msg, dict):
                            # dict æ ¼å¼ï¼ˆå…¼å®¹æ—§æ•°æ®ï¼‰
                            if "role" in msg:
                                messages.append(
                                    {"role": msg["role"], "content": msg.get("content", "")}
                                )
                            elif "type" in msg and "data" in msg:
                                msg_data = msg.get("data", {})
                                role = "user" if msg.get("type") == "human" else "assistant"
                                content = (
                                    msg_data.get("content", "")
                                    if isinstance(msg_data, dict)
                                    else str(msg_data)
                                )
                                messages.append({"role": role, "content": content})
                        elif hasattr(msg, "type") and hasattr(msg, "content"):
                            # LangChain æ¶ˆæ¯å¯¹è±¡ï¼ˆæ–°æ ¼å¼ï¼‰
                            role = "user" if msg.type == "human" else "assistant"
                            messages.append({"role": role, "content": str(msg.content)})

            return {
                "thread_id": thread_id,
                "messages": messages,
                "has_history": len(messages) > 0,
            }
        except Exception as e:
            logger.warning("No history found for thread", thread_id=thread_id, error=str(e))
            return {
                "thread_id": thread_id,
                "messages": [],
                "has_history": False,
            }
        finally:
            # å½’è¿˜æ•°æ®åº“è¿æ¥åˆ°è¿æ¥æ± 
            if conn:
                try:
                    await checkpointer_manager._pool.putconn(conn)
                except Exception as e:
                    logger.warning("Failed to return connection to pool", error=str(e))

    except Exception as e:
        logger.error("Failed to get chat messages", thread_id=thread_id, error=str(e))
        raise HTTPException(status_code=500, detail=f"Failed to fetch chat history: {str(e)}")


@router.get("/chat")
async def chat_sse_endpoint(
    message: str = "",
    project_id: str = "",
    thread_id: str = "",
    user_id: str = "dev-user",
    node_id: str = "",
):
    """
    SSE chat endpoint for streaming responses.
    Supports EventSource from frontend.
    """
    from fastapi.responses import StreamingResponse
    import asyncio
    import json

    async def event_generator():
        try:
            # æ„å»ºè¯·æ±‚
            request = ChatRequest(
                user_id=user_id,
                project_id=project_id or None,
                session_id=thread_id or None,
                message=message if message else None,
            )

            # åˆ›å»ºåˆå§‹çŠ¶æ€
            state = create_initial_state(
                user_id=user_id,
                project_id=project_id or str(uuid.uuid4()),
                thread_id=thread_id or str(uuid.uuid4()),
            )

            # å†·å¯åŠ¨æ£€æµ‹ - è®¾ç½®æ ‡å¿—ä½è®© LangGraph å¤„ç†
            is_cold_start = not message or is_cold_start_message(message)

            if is_cold_start:
                # æ ‡è®°ä¸ºå†·å¯åŠ¨ï¼Œè®© LangGraph çš„ cold_start èŠ‚ç‚¹å¤„ç†
                state["is_cold_start"] = True
                # ä¸ç›´æ¥è¿”å›ï¼Œç»§ç»­èµ°ä¸‹é¢çš„ LangGraph æµç¨‹
            else:
                state["is_cold_start"] = False

            # æ­£å¸¸æµç¨‹
            if message:
                from langchain_core.messages import HumanMessage

                state["messages"] = [HumanMessage(content=message)]
                state["user_input"] = message

            # è·å– graph
            graph = await get_graph_for_request()

            # å‡†å¤‡ config
            config = {
                "configurable": {
                    "thread_id": thread_id or str(uuid.uuid4()),
                }
            }

            # å‘é€èŠ‚ç‚¹å¼€å§‹äº‹ä»¶
            yield f"data: {json.dumps({'type': 'node_start', 'node': 'router', 'desc': 'æ­£åœ¨åˆ†ææ‚¨çš„è¯·æ±‚...'})}\n\n"
            await asyncio.sleep(0.1)

            # è¿è¡Œ graph
            result = await graph.ainvoke(state, config)

            # å‘é€èŠ‚ç‚¹ç»“æŸäº‹ä»¶
            yield f"data: {json.dumps({'type': 'node_end', 'node': 'router'})}\n\n"
            await asyncio.sleep(0.1)

            # æå–å“åº”å†…å®¹
            messages = result.get("messages", [])
            ai_content = ""

            if messages:
                for msg in reversed(messages):
                    if hasattr(msg, "content") and msg.content:
                        ai_content = msg.content
                        break

            # æ¸…ç† AI å†…å®¹ï¼ˆæå– ui_feedback æˆ– thought_processï¼‰
            def extract_display_content(content: str) -> str:
                """ä» Master Router JSON ä¸­æå–å¯æ˜¾ç¤ºå†…å®¹"""
                if not content or not isinstance(content, str):
                    return content or ""

                content = content.strip()
                if content.startswith("{") and (
                    '"ui_feedback"' in content or '"thought_process"' in content
                ):
                    try:
                        parsed = json.loads(content)
                        if parsed and isinstance(parsed, dict):
                            # ä¼˜å…ˆæå– ui_feedback
                            ui_feedback = parsed.get("ui_feedback")
                            if ui_feedback and isinstance(ui_feedback, str) and ui_feedback.strip():
                                return ui_feedback.strip()

                            # å¦‚æœæ²¡æœ‰ ui_feedbackï¼Œå°è¯•æå– thought_process
                            thought_process = parsed.get("thought_process")
                            if (
                                thought_process
                                and isinstance(thought_process, str)
                                and thought_process.strip()
                            ):
                                return thought_process.strip()
                    except (json.JSONDecodeError, TypeError):
                        pass

                return content

            display_content = extract_display_content(ai_content)

            # åˆ†è¯å‘é€ï¼ˆæ¨¡æ‹Ÿæµå¼è¾“å‡ºï¼‰
            if display_content:
                words = display_content.split()
                for i, word in enumerate(words):
                    yield f"data: {json.dumps({'type': 'token', 'content': word + ' '})}\n\n"
                    if i < len(words) - 1:
                        await asyncio.sleep(0.05)

            # å‘é€å®Œæˆäº‹ä»¶
            content_status = get_content_status(result)
            yield f"data: {
                json.dumps(
                    {
                        'type': 'done',
                        'state': {
                            'messages': [{'role': 'ai', 'content': ai_content}]
                            if ai_content
                            else [],
                            'thread_id': thread_id,
                            'content_status': content_status,
                        },
                    }
                )
            }\n\n"

        except Exception as e:
            logger.error("SSE chat error", error=str(e))
            yield f"data: {json.dumps({'type': 'error', 'message': str(e)})}\n\n"

    return StreamingResponse(
        event_generator(),
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
            "X-Accel-Buffering": "no",
        },
    )
